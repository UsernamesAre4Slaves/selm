{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed-Precision Training in SELM\n",
    "\n",
    "This notebook demonstrates the use of mixed-precision training in the SELM model. Mixed-precision training allows for faster computation and reduced memory usage by performing certain operations in lower precision (FP16) while maintaining model accuracy by keeping sensitive operations in full precision (FP32).\n",
    "\n",
    "We will utilize PyTorch’s `torch.cuda.amp` module for automatic mixed-precision training and compare the performance and memory utilization against standard training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.cuda import amp\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from src.model.transformer import SELMTransformer\n",
    "from src.tasks.text_classification import TextClassificationDataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset and Initialize Model\n",
    "\n",
    "We will load a text classification dataset and initialize the SELM model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_dataset = TextClassificationDataset('data/processed/train_data.csv')\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize the SELM model\n",
    "model = SELMTransformer(config_path='config/model_config.yaml')\n",
    "model = model.to('cuda')  # Move model to GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Optimizer and Loss Function\n",
    "\n",
    "We’ll define the optimizer (Adam) and the loss function (cross-entropy) for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Training Loop (FP32 Precision)\n",
    "\n",
    "We’ll first define a standard training loop using full precision (FP32) to compare it against the mixed-precision method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard training loop (FP32)\n",
    "def train_fp32():\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return time.time() - start_time\n",
    "\n",
    "# Run standard FP32 training\n",
    "fp32_time = train_fp32()\n",
    "print(f\"FP32 Training Time: {fp32_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed-Precision Training Loop (FP16 Precision)\n",
    "\n",
    "Next, we define the mixed-precision training loop using `torch.cuda.amp` for automatic mixed precision. This allows some operations to run in FP16, while others remain in FP32 to preserve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed-precision training loop (FP16)\n",
    "scaler = amp.GradScaler()  # For scaling gradients in FP16\n",
    "\n",
    "def train_fp16():\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Use autocast for mixed precision\n",
    "        with amp.autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # Scale loss for mixed-precision\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    return time.time() - start_time\n",
    "\n",
    "# Run mixed-precision FP16 training\n",
    "fp16_time = train_fp16()\n",
    "print(f\"FP16 Mixed-Precision Training Time: {fp16_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Memory Usage\n",
    "\n",
    "To compare memory usage between FP32 and FP16 training, we can measure the peak GPU memory usage for both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get memory usage on GPU\n",
    "def get_gpu_memory():\n",
    "    return torch.cuda.max_memory_allocated() / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "# Get memory usage for FP32 training\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "train_fp32()\n",
    "fp32_memory = get_gpu_memory()\n",
    "print(f\"FP32 Memory Usage: {fp32_memory:.2f} MB\")\n",
    "\n",
    "# Get memory usage for FP16 training\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "train_fp16()\n",
    "fp16_memory = get_gpu_memory()\n",
    "print(f\"FP16 Memory Usage: {fp16_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Results\n",
    "\n",
    "Let’s compare the training time and memory usage for both standard FP32 training and mixed-precision FP16 training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out performance comparison\n",
    "print(f\"FP32 Training Time: {fp32_time:.2f} seconds, FP32 Memory: {fp32_memory:.2f} MB\")\n",
    "print(f\"FP16 Mixed-Precision Training Time: {fp16_time:.2f} seconds, FP16 Memory: {fp16_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Mixed-precision training offers significant advantages in terms of both training speed and memory utilization. By utilizing FP16 operations where appropriate, we can achieve faster training times and lower memory consumption while maintaining the accuracy of the SELM model.\n",
    "\n",
    "In this notebook, we demonstrated how to integrate mixed-precision training into the SELM model, leveraging PyTorch’s `torch.cuda.amp` for automatic mixed precision. We observed that training with mixed precision can dramatically reduce the memory footprint and speed up training without sacrificing performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
