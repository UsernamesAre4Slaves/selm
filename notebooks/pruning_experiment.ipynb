{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELM Pruning Experiment\n",
    "\n",
    "In this notebook, we will experiment with model pruning and quantization for SELM. We will first prune the model to reduce its size and then quantize it to optimize performance for deployment.\n",
    "\n",
    "## 1. Setup\n",
    "Import necessary libraries and load the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.quantization as quant\n",
    "\n",
    "# Load the trained model\n",
    "model_path = 'model_output/trained_model/'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prune the Model\n",
    "Apply pruning to the model's parameters to reduce its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply pruning\n",
    "def prune_model(model, amount=0.2):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "\n",
    "prune_model(model, amount=0.2)\n",
    "\n",
    "# Check the pruning status\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        print(f'{name} pruned weights: {module.weight_mask.sum().item() / module.weight_mask.numel()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quantize the Model\n",
    "Quantize the pruned model to reduce its size further and improve performance for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare model for quantization\n",
    "model = torch.quantization.convert(model, inplace=False)\n",
    "\n",
    "# Test the quantized model\n",
    "def test_model(model, dataloader):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs, labels = batch['input_ids'], batch['labels']\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# Create dataloader for evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "test_dataloader = DataLoader(tokenized_dataset['test'], batch_size=16)\n",
    "\n",
    "accuracy = test_model(model, test_dataloader)\n",
    "print(f'Quantized Model Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save the Pruned and Quantized Model\n",
    "Save the pruned and quantized model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "quantized_model_path = 'model_output/quantized_model/'\n",
    "model.save_pretrained(quantized_model_path)\n",
    "print(f'Quantized model saved to {quantized_model_path}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
