# Model configuration
model_name: "distilbert-base-uncased"  # Pretrained model to use
num_labels: 2  # Number of labels for classification task

# Training configuration
learning_rate: 5e-5  # Initial learning rate
batch_size: 16  # Batch size for training
num_epochs: 5  # Number of training epochs
max_seq_length: 128  # Maximum sequence length for tokenization

# Optimization configuration
optimizer:
  type: "AdamW"  # Optimizer type
  weight_decay: 0.01  # Weight decay for optimizer

# Data configuration
dataset_name: "glue"  # Dataset name (e.g., GLUE)
dataset_config_name: "mrpc"  # Specific dataset configuration (e.g., MRPC)

# Logging and output configuration
logging:
  level: "info"  # Logging level (e.g., info, debug, warning, error)
  save_steps: 500  # Number of steps between saving checkpoints
  output_dir: "model_output/"  # Directory to save model checkpoints

# Evaluation configuration
evaluation:
  metrics: ["accuracy"]  # Metrics to compute during evaluation
  eval_steps: 500  # Number of steps between evaluations

# Pruning and Quantization configuration
pruning:
  enabled: false  # Whether pruning is enabled
  strategy: "magnitude"  # Pruning strategy

quantization:
  enabled: false  # Whether quantization is enabled
  dtype: "qint8"  # Quantization dtype (e.g., qint8 for 8-bit quantization)
